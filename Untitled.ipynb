{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2490 lines.\n"
     ]
    }
   ],
   "source": [
    "# load data set\n",
    "\n",
    "samples = []\n",
    "for i in range(1,510):\n",
    "    if i < 10:\n",
    "        i = \"00\" + str(i)\n",
    "    elif i < 100:\n",
    "        i = \"0\" + str(i)\n",
    "    \n",
    "    with open(\"data/bbc/business/{}.txt\".format(str(i))) as fh:\n",
    "        lines = fh.read().strip().split('\\n')\n",
    "        samples.append(lines[0])\n",
    "        \n",
    "        samples.append(\" \".join(lines[2].split(' ')[0:6]))\n",
    "        samples.append(\" \".join(lines[4].split(' ')[0:6]))\n",
    "        try:\n",
    "            samples.append(\" \".join(lines[6].split(' ')[0:6]))\n",
    "            samples.append(\" \".join(lines[8].split(' ')[0:6]))\n",
    "            samples.append(\" \".join(lines[10].split(' ')[0:6]))\n",
    "        except:\n",
    "            pass\n",
    " \n",
    "print(\"Loaded {} lines.\".format(len(samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The US created fewer jobs than\n"
     ]
    }
   ],
   "source": [
    "print(samples[29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load GloVe encodings\n",
    "\n",
    "import torch\n",
    "\n",
    "dimensions = 50\n",
    "\n",
    "print(\"Loading word vectors...\")\n",
    "embeddings_index = {}\n",
    "word_index = {}\n",
    "\n",
    "with open(\"data/glove{}.txt\".format(dimensions), 'r') as fh:\n",
    "    for line in fh:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = list(map(lambda x: float(x), values[1:])) # torch.FloatTensor(list(map(lambda x: float(x), values[1:])))\n",
    "        embeddings_index[word] = coefs\n",
    "        #key = list(map(lambda x: float(\"{0:.1f}\".format(x)), coefs))\n",
    "       \n",
    "        word_index[torch.FloatTensor(coefs)] = word\n",
    "        \n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not lookup embedding for word: '$280'\n",
      "could not lookup embedding for word: '$280'\n",
      "could not lookup embedding for word: '£155'\n",
      "could not lookup embedding for word: '43.305'\n",
      "could not lookup embedding for word: 'agroflora'\n",
      "could not lookup embedding for word: 'agroflora'\n",
      "could not lookup embedding for word: 'illva'\n",
      "could not lookup embedding for word: 'illva'\n",
      "could not lookup embedding for word: 'illva'\n",
      "could not lookup embedding for word: '£15.4m'\n",
      "could not lookup embedding for word: '$53'\n",
      "could not lookup embedding for word: '$4'\n",
      "could not lookup embedding for word: 're-listing'\n",
      "could not lookup embedding for word: list assignment index out of range\n",
      "could not lookup embedding for word: \"didn't\"\n",
      "could not lookup embedding for word: '418bn-rand'\n",
      "could not lookup embedding for word: 'bmw-brand'\n",
      "could not lookup embedding for word: '80,600'\n",
      "could not lookup embedding for word: 'ex-boss'\n",
      "could not lookup embedding for word: 'ex-head'\n",
      "could not lookup embedding for word: \"won't\"\n",
      "could not lookup embedding for word: \"we're\"\n",
      "could not lookup embedding for word: 'turkey-iran'\n",
      "could not lookup embedding for word: '$6'\n",
      "could not lookup embedding for word: '£3.2'\n",
      "could not lookup embedding for word: 'fiat-gm'\n",
      "could not lookup embedding for word: '$16'\n",
      "could not lookup embedding for word: '$102.6'\n",
      "could not lookup embedding for word: '$280'\n",
      "could not lookup embedding for word: '£80,000'\n",
      "could not lookup embedding for word: 'ex-aide'\n",
      "could not lookup embedding for word: 'like-for-like'\n",
      "could not lookup embedding for word: 'german-us'\n",
      "could not lookup embedding for word: 'carl-henric'\n",
      "could not lookup embedding for word: 'widely-expected'\n",
      "could not lookup embedding for word: '$515m'\n",
      "could not lookup embedding for word: '$28'\n",
      "could not lookup embedding for word: 'inoni'\n",
      "could not lookup embedding for word: 'sell-offs'\n",
      "could not lookup embedding for word: 'rate-setting'\n",
      "could not lookup embedding for word: '$50'\n",
      "could not lookup embedding for word: 'accura'\n",
      "could not lookup embedding for word: '$50'\n",
      "could not lookup embedding for word: '$11'\n",
      "could not lookup embedding for word: '$6'\n",
      "could not lookup embedding for word: '$6'\n",
      "could not lookup embedding for word: '£3.12'\n",
      "could not lookup embedding for word: 'uk-dutch'\n",
      "could not lookup embedding for word: '$2'\n",
      "could not lookup embedding for word: 'fiat-gm'\n",
      "could not lookup embedding for word: 'ex-boeing'\n",
      "could not lookup embedding for word: '$180'\n",
      "could not lookup embedding for word: '$20'\n",
      "could not lookup embedding for word: '$20'\n",
      "could not lookup embedding for word: '£10'\n",
      "could not lookup embedding for word: 'frankfurt-based'\n",
      "could not lookup embedding for word: '£1.3'\n",
      "could not lookup embedding for word: '537m'\n",
      "could not lookup embedding for word: 'buy-to-let'\n",
      "could not lookup embedding for word: list assignment index out of range\n",
      "could not lookup embedding for word: '$6.75'\n",
      "could not lookup embedding for word: '£3.6'\n",
      "could not lookup embedding for word: 'post-christmas'\n",
      "could not lookup embedding for word: 'post-christmas'\n",
      "could not lookup embedding for word: 'record-busting'\n",
      "could not lookup embedding for word: 'krivorizhstal'\n",
      "could not lookup embedding for word: 'belt-tightening'\n",
      "could not lookup embedding for word: 'with-profits'\n",
      "could not lookup embedding for word: 'with-profits'\n",
      "could not lookup embedding for word: 're-orders'\n",
      "could not lookup embedding for word: 'matshushita'\n",
      "could not lookup embedding for word: '£1'\n",
      "could not lookup embedding for word: 'post-enron'\n",
      "could not lookup embedding for word: '300-shareholder'\n",
      "could not lookup embedding for word: 'moscow-backed'\n",
      "could not lookup embedding for word: 'interest-payment'\n",
      "could not lookup embedding for word: '$1.8m'\n",
      "could not lookup embedding for word: 'like-for-like'\n",
      "could not lookup embedding for word: 'like-for-like'\n",
      "could not lookup embedding for word: 'paion'\n",
      "could not lookup embedding for word: 'long-living'\n",
      "could not lookup embedding for word: 'card-not-present'\n",
      "could not lookup embedding for word: '£42m'\n",
      "could not lookup embedding for word: '£3.3'\n",
      "could not lookup embedding for word: ''\n",
      "could not lookup embedding for word: 'interest-sensitive'\n",
      "could not lookup embedding for word: '$2.58'\n",
      "could not lookup embedding for word: '£1.38'\n",
      "could not lookup embedding for word: '$1.75'\n",
      "could not lookup embedding for word: 'post-quake'\n",
      "could not lookup embedding for word: '£398m'\n",
      "could not lookup embedding for word: 'split-caps'\n",
      "could not lookup embedding for word: '£194m'\n",
      "could not lookup embedding for word: '$100m'\n",
      "could not lookup embedding for word: 'technology-dominated'\n",
      "could not lookup embedding for word: '$1'\n",
      "could not lookup embedding for word: '$1'\n",
      "could not lookup embedding for word: '$40.15'\n",
      "could not lookup embedding for word: 'post-takeover'\n",
      "could not lookup embedding for word: 'toshitsune'\n",
      "could not lookup embedding for word: ''\n",
      "could not lookup embedding for word: '$25'\n",
      "could not lookup embedding for word: '$1'\n",
      "could not lookup embedding for word: '$1'\n",
      "could not lookup embedding for word: 'phytopharm'\n",
      "could not lookup embedding for word: 'phytopharm'\n",
      "could not lookup embedding for word: 'plane-making'\n",
      "could not lookup embedding for word: 'anglo-swedish'\n",
      "could not lookup embedding for word: '$9'\n",
      "could not lookup embedding for word: 's&n'\n",
      "could not lookup embedding for word: 's&n'\n",
      "could not lookup embedding for word: 'al-jibouri'\n",
      "could not lookup embedding for word: 'contrack'\n",
      "could not lookup embedding for word: '$1.8'\n",
      "could not lookup embedding for word: 'quick-growing'\n",
      "could not lookup embedding for word: 'quick-growing'\n",
      "could not lookup embedding for word: '$1.3516'\n",
      "could not lookup embedding for word: 'people...there'\n",
      "could not lookup embedding for word: '$10'\n",
      "could not lookup embedding for word: 'april-july'\n",
      "could not lookup embedding for word: list assignment index out of range\n",
      "could not lookup embedding for word: '$2.86'\n",
      "could not lookup embedding for word: 'feud-hit'\n",
      "could not lookup embedding for word: 'post-quake'\n",
      "could not lookup embedding for word: '£398m'\n",
      "could not lookup embedding for word: 'faster-than-expected'\n",
      "could not lookup embedding for word: 'car-making'\n",
      "could not lookup embedding for word: '$100m-share'\n",
      "could not lookup embedding for word: 'technology-dominated'\n",
      "could not lookup embedding for word: 'alfa-eco'\n",
      "could not lookup embedding for word: 'leuven-based'\n",
      "could not lookup embedding for word: 'boix-vives'\n",
      "could not lookup embedding for word: '$1.3652'\n",
      "could not lookup embedding for word: '141.60'\n",
      "could not lookup embedding for word: '$14m'\n",
      "could not lookup embedding for word: 'lee-harwood'\n",
      "could not lookup embedding for word: '$1.3'\n",
      "could not lookup embedding for word: '$1.3'\n",
      "could not lookup embedding for word: '£691m'\n",
      "could not lookup embedding for word: '$1.5m'\n",
      "could not lookup embedding for word: 'chemicals-and-crops'\n",
      "could not lookup embedding for word: '$54m'\n",
      "could not lookup embedding for word: 'us-german'\n",
      "could not lookup embedding for word: 'sell-offs'\n",
      "could not lookup embedding for word: 'sell-offs'\n",
      "could not lookup embedding for word: '$168m'\n",
      "could not lookup embedding for word: 'odpm'\n",
      "could not lookup embedding for word: '$1m'\n",
      "could not lookup embedding for word: '$1m'\n",
      "could not lookup embedding for word: '£522,000'\n",
      "could not lookup embedding for word: '$40'\n",
      "could not lookup embedding for word: '£21'\n",
      "could not lookup embedding for word: '$5.4'\n",
      "could not lookup embedding for word: 'eu-us'\n",
      "could not lookup embedding for word: '$300m'\n",
      "could not lookup embedding for word: '225.79'\n",
      "could not lookup embedding for word: '$55.5'\n",
      "could not lookup embedding for word: '£29'\n",
      "could not lookup embedding for word: 'trouble-hit'\n",
      "could not lookup embedding for word: \"won't\"\n",
      "could not lookup embedding for word: 'trade-in-services'\n",
      "could not lookup embedding for word: 'a$4.17-a-share'\n",
      "could not lookup embedding for word: 'straight-talking'\n",
      "could not lookup embedding for word: \"we'll\"\n",
      "could not lookup embedding for word: '$36m'\n",
      "could not lookup embedding for word: 'back-tax'\n",
      "could not lookup embedding for word: '$630m'\n",
      "could not lookup embedding for word: '£481.5m'\n",
      "could not lookup embedding for word: '$18.6'\n",
      "could not lookup embedding for word: '$3.5'\n",
      "could not lookup embedding for word: '$368'\n",
      "could not lookup embedding for word: 'post-enron'\n",
      "could not lookup embedding for word: '300-shareholder'\n",
      "could not lookup embedding for word: '22,953'\n",
      "could not lookup embedding for word: 'colludes'\n",
      "could not lookup embedding for word: '$45'\n",
      "could not lookup embedding for word: 'china-published'\n",
      "could not lookup embedding for word: 'china-published'\n",
      "could not lookup embedding for word: 'scandal-hit'\n",
      "could not lookup embedding for word: list assignment index out of range\n",
      "could not lookup embedding for word: 'ex-boss'\n",
      "done embedding sentences\n"
     ]
    }
   ],
   "source": [
    "# encode each sample as array of embedded words\n",
    "\n",
    "import torch\n",
    "\n",
    "seq_len = 6 #1025   # number of words in sentence\n",
    "embedded_sentences = []\n",
    "\n",
    "\n",
    "\n",
    "for sample in samples:\n",
    "    \n",
    "    sentence = [[0.0]*dimensions]*(seq_len+1) #torch.FloatTensor(1025, dimensions).zero_()\n",
    "    #sentence[0] = torch.ones_like(torch.FloatTensor(1, dimensions)) # first element in sentence is all ones => start token\n",
    "    ind = 0 #1 \n",
    "    \n",
    "    for word in sample.split(' '):\n",
    "        word = word.lower().strip('(,.;!?:\"').strip(\"'\").strip(\")\")\n",
    "        if word.startswith('\"'):\n",
    "            word = word[1:]\n",
    "        if word.endswith(\"'s\"):\n",
    "            word = word[0:-2]\n",
    "        elif word.endswith(\"bn\"):\n",
    "            words = [word[0:-2], \"billion\"]\n",
    "        elif word.endswith(\"%\"):\n",
    "            words = [word[0:-1], \"percent\"]\n",
    "        else:\n",
    "            words = [word]\n",
    "        \n",
    "        try:\n",
    "            for word in words:\n",
    "                coefs = embeddings_index[word]\n",
    "                sentence[ind] = coefs\n",
    "                ind+=1\n",
    "                \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"could not lookup embedding for word: {}\".format(e))\n",
    "    \n",
    "        \n",
    "    embedded_sentences.append(sentence)\n",
    "\n",
    "print(\"done embedding sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# Random item from a list\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "# Get a random line\n",
    "def randomTrainingPair():\n",
    "    line = randomChoice(embedded_sentences) # has shape (1025, 300)\n",
    "    l = line[0:-1]\n",
    "    v = torch.FloatTensor(l)\n",
    "    input = Variable(v.view(seq_len, 1, dimensions))#, requires_grad=True)    \n",
    "    t = torch.FloatTensor(line[1:])\n",
    "    target = Variable(t.view(seq_len, 1, dimensions))               \n",
    "    return (input, target)\n",
    "\n",
    "# Timing runtime of network operations\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 24s (1000 2%) 0.3006\n",
      "0m 47s (2000 4%) 0.3512\n",
      "1m 11s (3000 6%) 0.2737\n",
      "1m 34s (4000 8%) 0.2871\n",
      "1m 58s (5000 10%) 0.3995\n",
      "2m 21s (6000 12%) 0.3229\n",
      "2m 44s (7000 14%) 0.3322\n",
      "3m 6s (8000 16%) 0.4146\n",
      "3m 29s (9000 18%) 0.2780\n",
      "3m 53s (10000 20%) 0.2948\n",
      "4m 17s (11000 22%) 0.3313\n",
      "4m 40s (12000 24%) 0.3239\n",
      "5m 4s (13000 26%) 0.5752\n",
      "5m 30s (14000 28%) 0.2998\n",
      "5m 53s (15000 30%) 0.2348\n",
      "6m 15s (16000 32%) 0.4798\n",
      "6m 40s (17000 34%) 0.3276\n",
      "7m 3s (18000 36%) 0.4067\n",
      "7m 27s (19000 38%) 0.3719\n",
      "7m 51s (20000 40%) 0.3797\n",
      "8m 14s (21000 42%) 0.2740\n",
      "8m 38s (22000 44%) 0.3213\n",
      "9m 1s (23000 46%) 0.2772\n",
      "9m 25s (24000 48%) 0.3194\n",
      "9m 48s (25000 50%) 0.3029\n",
      "10m 12s (26000 52%) 0.4203\n",
      "10m 36s (27000 54%) 0.3239\n",
      "10m 59s (28000 56%) 0.2713\n",
      "11m 23s (29000 57%) 0.3491\n",
      "11m 52s (30000 60%) 0.3167\n",
      "12m 21s (31000 62%) 0.3149\n",
      "12m 50s (32000 64%) 0.3824\n",
      "13m 19s (33000 66%) 0.2977\n",
      "13m 50s (34000 68%) 0.2849\n",
      "14m 19s (35000 70%) 0.3896\n",
      "14m 49s (36000 72%) 0.3109\n",
      "15m 19s (37000 74%) 0.3229\n",
      "15m 49s (38000 76%) 0.3146\n",
      "16m 18s (39000 78%) 0.2205\n",
      "16m 45s (40000 80%) 0.2370\n",
      "17m 13s (41000 82%) 0.3338\n",
      "17m 37s (42000 84%) 0.3462\n",
      "18m 1s (43000 86%) 0.3594\n",
      "18m 25s (44000 88%) 0.4054\n",
      "18m 49s (45000 90%) 0.4851\n",
      "19m 13s (46000 92%) 0.3824\n",
      "19m 37s (47000 94%) 0.3600\n",
      "20m 1s (48000 96%) 0.5103\n",
      "20m 25s (49000 98%) 0.4657\n",
      "20m 55s (50000 100%) 0.3918\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "batch_size = 1\n",
    "input_size = dimensions # length of glove vectors\n",
    "num_layers = 2\n",
    "hidden_size = input_size # must be same?\n",
    "\n",
    "gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=.2)\n",
    "\n",
    "# train network\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "def train(input, target, hidden):\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i in range(input.size()[0]): # 1024 words in sentence\n",
    "        #print(i)\n",
    "        output, hidden = gru(input, hidden)\n",
    "        loss_var = criterion(output, target)\n",
    "        total_loss += loss_var.data[0]\n",
    "        \n",
    "    \n",
    "    loss_var.backward()\n",
    "    \n",
    "    for p in gru.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "   \n",
    "    return total_loss / input.size()[0] # average loss\n",
    "  \n",
    "\n",
    "\n",
    "start = time.time()\n",
    "        \n",
    "n_iters = 50000\n",
    "print_every = 1000\n",
    "#plot_every = 100\n",
    "\n",
    "\n",
    "hidden = Variable(torch.zeros(num_layers, 1, hidden_size)) # for init only\n",
    "for iter in range(1, n_iters+1):\n",
    "    # input shape: (seq_len, batch, input_size)\n",
    "    input_tensor, target_tensor = randomTrainingPair() # random embedded sentence\n",
    "    \n",
    "    loss = train(input_tensor, target_tensor, hidden)\n",
    "    \n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "    #if iter % plot_every == 0:\n",
    "     #   all_losses.append(total_loss / plot_every)\n",
    "     #   total_loss = 0\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 5s (100000 25%)\n",
      "0m 11s (200000 50%)\n",
      "exception during embedded word lookup: float division by zero\n",
      "0m 16s (300000 75%)\n",
      "0m 22s (400000 100%)\n",
      "Output:  ironically coincidentally bdb94 ironically str95bb str95bb ironically mo95 \n"
     ]
    }
   ],
   "source": [
    "max_length = 6\n",
    "\n",
    "#eos = torch.FloatTensor(1,dimensions).zero_()\n",
    "\n",
    "def round_floats(float_num, dummy):\n",
    "    return float(\"{0:.2f}\".format(float_num))\n",
    "\n",
    "start_word  = \"ironically\"\n",
    "\n",
    "def generate():\n",
    "    # TODO pick random start word\n",
    "    input = Variable(torch.FloatTensor(embeddings_index[start_word]).view(1,1,dimensions)) # start word \n",
    "    \n",
    "    generated_sentence = [input.data]\n",
    "    hidden = Variable(torch.zeros(num_layers, 1, hidden_size)) # pick random vector from embeddings\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        output, hidden = gru(input, hidden)\n",
    "     \n",
    "        #if torch.equal(output.data.view(1,dimensions), eos):\n",
    "            #print(\"found eos after {} words\".format(i))\n",
    "            #break\n",
    "        \n",
    "        generated_sentence.append(output.data)  # storing embedded words for now\n",
    "        input = output\n",
    "\n",
    "    return generated_sentence\n",
    "    \n",
    "\n",
    "print_every = 100000\n",
    "def translate(embedded_sentence):\n",
    "    iter = 0\n",
    "    start = time.time()\n",
    "    closest_matches = [[0.0,0,\"???\"]] * len(embedded_sentence)  # TODO \n",
    "        \n",
    "    # go through list of words in order of occurance probability\n",
    "    for vector, word in word_index.items():\n",
    "        iter += 1\n",
    "        if iter % print_every == 0:\n",
    "            print('%s (%d %d%%)' % (timeSince(start), iter, iter / 400000 * 100))\n",
    "            \n",
    "        #cursor_tensor_key = emb_word_tensor.map_(torch.FloatTensor([0]), round_floats)\n",
    "        \n",
    "        # check distance for each word in sentence\n",
    "        ind = 0\n",
    "        for emb_word_tensor in embedded_sentence:\n",
    "            try:\n",
    "                # calculate distance\n",
    "                \n",
    "                dist = 1.0 / torch.sum(torch.pow(vector.sub(emb_word_tensor), 2))\n",
    "                \n",
    "                if dist > closest_matches[ind][0]:\n",
    "                    \n",
    "                    closest_matches[ind] = [dist, vector, word]\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(\"exception during embedded word lookup: {}\".format(e))\n",
    "          \n",
    "            ind += 1\n",
    "    \n",
    "    sentence = \"\"\n",
    "    for [_, _, word] in closest_matches:\n",
    "        sentence = sentence + str(word) + \" \"\n",
    "                       \n",
    "    return sentence\n",
    "\n",
    "print(\"Output: \", start_word, translate(generate()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
