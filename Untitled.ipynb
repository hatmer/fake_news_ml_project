{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 510 lines.\n"
     ]
    }
   ],
   "source": [
    "# load data set\n",
    "\n",
    "samples = []\n",
    "for i in range(1,511):\n",
    "    if i < 10:\n",
    "        i = \"00\" + str(i)\n",
    "    elif i < 100:\n",
    "        i = \"0\" + str(i)\n",
    "    \n",
    "    with open(\"data/bbc/business/{}.txt\".format(str(i))) as fh:\n",
    "        lines = fh.read().strip().split('\\n')\n",
    "        samples.append(lines[0])\n",
    "        #samples.append(lines[0] + \" \" + lines[2] + \" \" + lines[4])\n",
    "\n",
    "print(\"Loaded {} lines.\".format(len(samples)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Court rejects $280bn tobacco case\n"
     ]
    }
   ],
   "source": [
    "print(samples[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load GloVe encodings\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"Loading word vectors...\")\n",
    "embeddings_index = {}\n",
    "word_index = {}\n",
    "\n",
    "with open(\"glove300.txt\", 'r') as fh:\n",
    "    for line in fh:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = list(map(lambda x: float(x), values[1:])) # torch.FloatTensor(list(map(lambda x: float(x), values[1:])))\n",
    "        embeddings_index[word] = coefs\n",
    "        #key = list(map(lambda x: float(\"{0:.1f}\".format(x)), coefs))\n",
    "       \n",
    "        word_index[torch.FloatTensor(coefs)] = word\n",
    "        \n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not lookup embedding for word: '$280'\n",
      "could not lookup embedding for word: 'illva'\n",
      "could not lookup embedding for word: '$53'\n",
      "could not lookup embedding for word: '$4'\n",
      "could not lookup embedding for word: 'ex-boss'\n",
      "could not lookup embedding for word: \"won't\"\n",
      "could not lookup embedding for word: 'turkey-iran'\n",
      "could not lookup embedding for word: 'fiat-gm'\n",
      "could not lookup embedding for word: '$16'\n",
      "could not lookup embedding for word: '$102.6'\n",
      "could not lookup embedding for word: '$280'\n",
      "could not lookup embedding for word: 'ex-aide'\n",
      "could not lookup embedding for word: '$515m'\n",
      "could not lookup embedding for word: '$28'\n",
      "could not lookup embedding for word: 'sell-offs'\n",
      "could not lookup embedding for word: '$50'\n",
      "could not lookup embedding for word: '$50'\n",
      "could not lookup embedding for word: '$11'\n",
      "could not lookup embedding for word: '$6'\n",
      "could not lookup embedding for word: '$2'\n",
      "could not lookup embedding for word: 'ex-boeing'\n",
      "could not lookup embedding for word: '$20'\n",
      "could not lookup embedding for word: list assignment index out of range\n",
      "could not lookup embedding for word: 'post-christmas'\n",
      "could not lookup embedding for word: 'post-enron'\n",
      "could not lookup embedding for word: 'long-living'\n",
      "could not lookup embedding for word: '£42m'\n",
      "could not lookup embedding for word: '£3.3'\n",
      "could not lookup embedding for word: 'post-quake'\n",
      "could not lookup embedding for word: 'split-caps'\n",
      "could not lookup embedding for word: '£194m'\n",
      "could not lookup embedding for word: '$100m'\n",
      "could not lookup embedding for word: '$1'\n",
      "could not lookup embedding for word: 'post-takeover'\n",
      "could not lookup embedding for word: '$25'\n",
      "could not lookup embedding for word: '$1'\n",
      "could not lookup embedding for word: 'phytopharm'\n",
      "could not lookup embedding for word: '$9'\n",
      "could not lookup embedding for word: 's&n'\n",
      "could not lookup embedding for word: '$1.8'\n",
      "could not lookup embedding for word: '$10'\n",
      "could not lookup embedding for word: list assignment index out of range\n",
      "could not lookup embedding for word: 'feud-hit'\n",
      "could not lookup embedding for word: 'post-quake'\n",
      "could not lookup embedding for word: '$100m-share'\n",
      "could not lookup embedding for word: '$14m'\n",
      "could not lookup embedding for word: '$1.3'\n",
      "could not lookup embedding for word: '$1.5m'\n",
      "could not lookup embedding for word: '$54m'\n",
      "could not lookup embedding for word: 'sell-offs'\n",
      "could not lookup embedding for word: '$168m'\n",
      "could not lookup embedding for word: '$1m'\n",
      "could not lookup embedding for word: '$5.4'\n",
      "could not lookup embedding for word: 'eu-us'\n",
      "could not lookup embedding for word: '$300m'\n",
      "could not lookup embedding for word: '$36m'\n",
      "could not lookup embedding for word: 'back-tax'\n",
      "could not lookup embedding for word: '$368'\n",
      "could not lookup embedding for word: 'post-enron'\n",
      "could not lookup embedding for word: 'colludes'\n",
      "could not lookup embedding for word: 'ex-boss'\n",
      "done embedding sentences\n"
     ]
    }
   ],
   "source": [
    "# encode each sample as array of embedded words\n",
    "\n",
    "import torch\n",
    "\n",
    "seq_len = 6 #1025   # number of words in sentence\n",
    "embedded_sentences = []\n",
    "\n",
    "dimensions = 300\n",
    "\n",
    "for sample in samples:\n",
    "    \n",
    "    sentence = [[0.0]*dimensions]*(seq_len+1) #torch.FloatTensor(1025, dimensions).zero_()\n",
    "    #sentence[0] = torch.ones_like(torch.FloatTensor(1, dimensions)) # first element in sentence is all ones => start token\n",
    "    ind = 0 #1 \n",
    "    \n",
    "    for word in sample.split(' '):\n",
    "        word = word.lower().strip('(,.;!?:\"').strip(\"'\").strip(\")\")\n",
    "        if word.startswith('\"'):\n",
    "            word = word[1:]\n",
    "        if word.endswith(\"'s\"):\n",
    "            word = word[0:-2]\n",
    "        elif word.endswith(\"bn\"):\n",
    "            words = [word[0:-2], \"billion\"]\n",
    "        elif word.endswith(\"%\"):\n",
    "            words = [word[0:-1], \"percent\"]\n",
    "        else:\n",
    "            words = [word]\n",
    "        \n",
    "        try:\n",
    "            for word in words:\n",
    "                coefs = embeddings_index[word]\n",
    "                sentence[ind] = coefs\n",
    "                ind+=1\n",
    "                \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"could not lookup embedding for word: {}\".format(e))\n",
    "    \n",
    "        \n",
    "    embedded_sentences.append(sentence)\n",
    "\n",
    "print(\"done embedding sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# Random item from a list\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "# Get a random line\n",
    "def randomTrainingPair():\n",
    "    line = randomChoice(embedded_sentences) # has shape (1025, 300)\n",
    "    l = line[0:-1]\n",
    "    v = torch.FloatTensor(l)\n",
    "    input = Variable(v.view(seq_len, 1, dimensions), requires_grad=True)\n",
    "    \n",
    "    t = torch.FloatTensor(line[1:])\n",
    "    target = Variable(t.view(seq_len, 1, dimensions))\n",
    "                  \n",
    "    return (input, target)\n",
    "\n",
    "# Timing runtime of network operations\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 43s (1000 10%) 0.4773\n",
      "3m 25s (2000 20%) 0.4335\n",
      "5m 6s (3000 30%) 0.7313\n",
      "6m 51s (4000 40%) 0.7664\n",
      "8m 33s (5000 50%) 0.5775\n",
      "10m 17s (6000 60%) 0.9244\n",
      "11m 59s (7000 70%) 0.8302\n",
      "13m 41s (8000 80%) 0.4965\n",
      "15m 28s (9000 90%) 0.2976\n",
      "17m 11s (10000 100%) 0.6287\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "batch_size = 1\n",
    "input_size = dimensions # length of glove vectors\n",
    "num_layers = 2\n",
    "hidden_size = input_size # must be same?\n",
    "\n",
    "gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=.2)\n",
    "\n",
    "# train network\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.0005\n",
    "\n",
    "\n",
    "# input : start token to last letter\n",
    "# target: first letter to EOS\n",
    "def train(input, target, hidden):\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i in range(input.size()[0]): # 1024 words in sentence\n",
    "        #print(i)\n",
    "        output, hidden = gru(input.detach(), hidden)\n",
    "        loss_var = criterion(output, target)\n",
    "        total_loss += loss_var.data[0]\n",
    "        \n",
    "    \n",
    "    loss_var.backward()\n",
    "   \n",
    "    return total_loss\n",
    "  \n",
    "\n",
    "\n",
    "start = time.time()\n",
    "        \n",
    "n_iters = 10000\n",
    "print_every = 1000\n",
    "#plot_every = 100\n",
    "\n",
    "\n",
    "hidden = Variable(torch.zeros(num_layers, 1, hidden_size)) # for init only\n",
    "for iter in range(1, n_iters+1):\n",
    "    # input shape: (seq_len, batch, input_size)\n",
    "    input_tensor, target_tensor = randomTrainingPair() # random embedded sentence\n",
    "    \n",
    "    loss = train(input_tensor, target_tensor, hidden)\n",
    "    \n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "    #if iter % plot_every == 0:\n",
    "     #   all_losses.append(total_loss / plot_every)\n",
    "     #   total_loss = 0\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not look up embedded word: float division by zero\n",
      "0m 5s (100000 25%)\n",
      "0m 11s (200000 50%)\n",
      "0m 16s (300000 75%)\n"
     ]
    }
   ],
   "source": [
    "max_length = 6\n",
    "\n",
    "eos = torch.FloatTensor(1,dimensions).zero_()\n",
    "\n",
    "def round_floats(float_num, dummy):\n",
    "    return float(\"{0:.2f}\".format(float_num))\n",
    "\n",
    "start_word  = \"court\"\n",
    "\n",
    "def generate():\n",
    "    # TODO pick random start word\n",
    "    input = Variable(torch.FloatTensor(embeddings_index[start_word]).view(1,1,dimensions)) # start word \n",
    "    \n",
    "    generated_sentence = [input.data]\n",
    "    hidden = Variable(torch.zeros(num_layers, 1, hidden_size)) # pick random vector from embeddings\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        output, hidden = gru(input, hidden)\n",
    "        #print(type(output.data))\n",
    "        #topv, topi = output.data.topk(1)\n",
    "        #topi = topi[0][0]\n",
    "        #print(topi)\n",
    "        if torch.equal(output.data.view(1,dimensions), eos):\n",
    "            print(\"found eos after {} words\".format(i))\n",
    "            break\n",
    "        else:\n",
    "            #embedded_word = all_letters[topi]\n",
    "            #next_input = Variable(torch.FloatTensor(output.data))\n",
    "            \n",
    "            generated_sentence.append(output.data)  # storing embedded words for now\n",
    "            \n",
    "            \n",
    "        input = output\n",
    "\n",
    "    return generated_sentence\n",
    "    \n",
    "\n",
    "print_every = 100000\n",
    "def translate(embedded_sentence):\n",
    "    iter = 0\n",
    "    start = time.time()\n",
    "    closest_matches = [[0.0,0,\"???\"]] * len(embedded_sentence)  # TODO \n",
    "        \n",
    "    # go through list of words in order of occurance probability\n",
    "    for vector, word in word_index.items():\n",
    "        iter += 1\n",
    "        if iter % print_every == 0:\n",
    "            print('%s (%d %d%%)' % (timeSince(start), iter, iter / 400000 * 100))\n",
    "            \n",
    "        #cursor_tensor_key = emb_word_tensor.map_(torch.FloatTensor([0]), round_floats)\n",
    "        \n",
    "        # check distance for each word in sentence\n",
    "        ind = 0\n",
    "        for emb_word_tensor in embedded_sentence:\n",
    "            try:\n",
    "                # calculate distance\n",
    "                \n",
    "                dist = 1.0 / torch.sum(torch.pow(vector.sub(emb_word_tensor), 2))\n",
    "                \n",
    "                if dist > closest_matches[ind][0]:\n",
    "                    \n",
    "                    closest_matches[ind] = [dist, vector, word]\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(\"could not look up embedded word: {}\".format(e))\n",
    "          \n",
    "            ind += 1\n",
    "    \n",
    "    sentence = \"\"\n",
    "    for [_, _, word] in closest_matches:\n",
    "        sentence = sentence + str(word) + \" \"\n",
    "                       \n",
    "    return sentence\n",
    "\n",
    "print(\"Output: \", start_word, translate(generate()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
