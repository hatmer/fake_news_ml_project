{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 510 lines.\n"
     ]
    }
   ],
   "source": [
    "# load data set\n",
    "\n",
    "samples = []\n",
    "for i in range(1,511):\n",
    "    if i < 10:\n",
    "        i = \"00\" + str(i)\n",
    "    elif i < 100:\n",
    "        i = \"0\" + str(i)\n",
    "    \n",
    "    with open(\"data/bbc/business/{}.txt\".format(str(i))) as fh:\n",
    "        lines = fh.read().strip().split('\\n')\n",
    "        samples.append(lines[0])\n",
    "        #samples.append(lines[0] + \" \" + lines[2] + \" \" + lines[4])\n",
    "\n",
    "print(\"Loaded {} lines.\".format(len(samples)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High fuel prices hit BA's profits\n"
     ]
    }
   ],
   "source": [
    "print(samples[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load GloVe encodings\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"Loading word vectors...\")\n",
    "embeddings_index = {}\n",
    "word_index = {}\n",
    "\n",
    "with open(\"glove300.txt\", 'r') as fh:\n",
    "    for line in fh:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = list(map(lambda x: float(x), values[1:])) # torch.FloatTensor(list(map(lambda x: float(x), values[1:])))\n",
    "        embeddings_index[word] = coefs\n",
    "        #key = list(map(lambda x: float(\"{0:.1f}\".format(x)), coefs))\n",
    "       \n",
    "        word_index[torch.FloatTensor(coefs)] = word\n",
    "        \n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not lookup embedding for word: '$280'\n",
      "could not lookup embedding for word: 'illva'\n",
      "could not lookup embedding for word: '$53'\n",
      "could not lookup embedding for word: '$4'\n",
      "could not lookup embedding for word: 'ex-boss'\n",
      "could not lookup embedding for word: \"won't\"\n",
      "could not lookup embedding for word: 'turkey-iran'\n",
      "could not lookup embedding for word: 'fiat-gm'\n",
      "could not lookup embedding for word: '$16'\n",
      "could not lookup embedding for word: '$102.6'\n",
      "could not lookup embedding for word: '$280'\n",
      "could not lookup embedding for word: 'ex-aide'\n",
      "could not lookup embedding for word: '$515m'\n",
      "could not lookup embedding for word: '$28'\n",
      "could not lookup embedding for word: 'sell-offs'\n",
      "could not lookup embedding for word: '$50'\n",
      "could not lookup embedding for word: '$50'\n",
      "could not lookup embedding for word: '$11'\n",
      "could not lookup embedding for word: '$6'\n",
      "could not lookup embedding for word: '$2'\n",
      "could not lookup embedding for word: 'ex-boeing'\n",
      "could not lookup embedding for word: '$20'\n",
      "could not lookup embedding for word: 'post-christmas'\n",
      "could not lookup embedding for word: 'post-enron'\n",
      "could not lookup embedding for word: 'long-living'\n",
      "could not lookup embedding for word: '£42m'\n",
      "could not lookup embedding for word: '£3.3'\n",
      "could not lookup embedding for word: 'post-quake'\n",
      "could not lookup embedding for word: 'split-caps'\n",
      "could not lookup embedding for word: '£194m'\n",
      "could not lookup embedding for word: '$100m'\n",
      "could not lookup embedding for word: '$1'\n",
      "could not lookup embedding for word: 'post-takeover'\n",
      "could not lookup embedding for word: '$25'\n",
      "could not lookup embedding for word: '$1'\n",
      "could not lookup embedding for word: 'phytopharm'\n",
      "could not lookup embedding for word: '$9'\n",
      "could not lookup embedding for word: 's&n'\n",
      "could not lookup embedding for word: '$1.8'\n",
      "could not lookup embedding for word: '$10'\n",
      "could not lookup embedding for word: 'feud-hit'\n",
      "could not lookup embedding for word: 'post-quake'\n",
      "could not lookup embedding for word: '$100m-share'\n",
      "could not lookup embedding for word: '$14m'\n",
      "could not lookup embedding for word: '$1.3'\n",
      "could not lookup embedding for word: '$1.5m'\n",
      "could not lookup embedding for word: '$54m'\n",
      "could not lookup embedding for word: 'sell-offs'\n",
      "could not lookup embedding for word: '$168m'\n",
      "could not lookup embedding for word: '$1m'\n",
      "could not lookup embedding for word: '$5.4'\n",
      "could not lookup embedding for word: 'eu-us'\n",
      "could not lookup embedding for word: '$300m'\n",
      "could not lookup embedding for word: '$36m'\n",
      "could not lookup embedding for word: 'back-tax'\n",
      "could not lookup embedding for word: '$368'\n",
      "could not lookup embedding for word: 'post-enron'\n",
      "could not lookup embedding for word: 'colludes'\n",
      "could not lookup embedding for word: 'ex-boss'\n",
      "done embedding sentences\n"
     ]
    }
   ],
   "source": [
    "# encode each sample as array of embedded words\n",
    "\n",
    "import torch\n",
    "\n",
    "seq_len = 8 #1025   # number of words in sentence\n",
    "embedded_sentences = []\n",
    "\n",
    "for sample in samples:\n",
    "    \n",
    "    sentence = [[0.0]*300]*(seq_len+1) #torch.FloatTensor(1025, 300).zero_()\n",
    "    #sentence[0] = torch.ones_like(torch.FloatTensor(1, 300)) # first element in sentence is all ones => start token\n",
    "    ind = 0 #1 \n",
    "    \n",
    "    for word in sample.split(' '):\n",
    "        word = word.lower().strip('(,.;!?:\"').strip(\"'\").strip(\")\")\n",
    "        if word.startswith('\"'):\n",
    "            word = word[1:]\n",
    "        if word.endswith(\"'s\"):\n",
    "            word = word[0:-2]\n",
    "        elif word.endswith(\"bn\"):\n",
    "            words = [word[0:-2], \"billion\"]\n",
    "        elif word.endswith(\"%\"):\n",
    "            words = [word[0:-1], \"percent\"]\n",
    "        else:\n",
    "            words = [word]\n",
    "        \n",
    "        try:\n",
    "            for word in words:\n",
    "                coefs = embeddings_index[word]\n",
    "                sentence[ind] = coefs\n",
    "                ind+=1\n",
    "                \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"could not lookup embedding for word: {}\".format(e))\n",
    "    \n",
    "        \n",
    "    embedded_sentences.append(sentence)\n",
    "\n",
    "print(\"done embedding sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Random item from a list\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "# Get a random line\n",
    "def randomTrainingPair():\n",
    "    line = randomChoice(embedded_sentences) # has shape (1025, 300)\n",
    "    l = line[0:-1]\n",
    "    v = torch.FloatTensor(l)\n",
    "    input = Variable(v.view(seq_len, 1, 300), requires_grad=True)\n",
    "    \n",
    "    t = torch.FloatTensor(line[1:])\n",
    "    target = Variable(t.view(seq_len, 1, 300))\n",
    "                  \n",
    "    return (input, target)\n",
    "\n",
    "# Timing runtime of network operations\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 53s (1000 10%) 0.4213\n",
      "1m 46s (2000 20%) 0.6092\n",
      "2m 54s (3000 30%) 0.8868\n",
      "4m 4s (4000 40%) 0.5635\n",
      "4m 56s (5000 50%) 0.8506\n",
      "5m 49s (6000 60%) 0.5414\n",
      "6m 45s (7000 70%) 0.5985\n",
      "7m 38s (8000 80%) 0.7610\n",
      "8m 32s (9000 90%) 0.6529\n",
      "9m 24s (10000 100%) 0.5678\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "batch_size = 1\n",
    "input_size = 300 # length of glove vectors\n",
    "num_layers = 2\n",
    "hidden_size = input_size # must be same?\n",
    "\n",
    "gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=.2)\n",
    "\n",
    "# train network\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.0005\n",
    "\n",
    "\n",
    "# input : start token to last letter\n",
    "# target: first letter to EOS\n",
    "def train(input, target, hidden):\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i in range(input.size()[0]): # 1024 words in sentence\n",
    "        #print(i)\n",
    "        output, hidden = gru(input.detach(), hidden.detach())\n",
    "        loss_var = criterion(output, target)\n",
    "        total_loss += loss_var.data[0]\n",
    "        \n",
    "    \n",
    "    loss_var.backward()\n",
    "   \n",
    "    return total_loss\n",
    "  \n",
    "\n",
    "\n",
    "start = time.time()\n",
    "        \n",
    "n_iters = 10000\n",
    "print_every = 1000\n",
    "#plot_every = 100\n",
    "\n",
    "for iter in range(1, n_iters+1):\n",
    "    # input shape: (seq_len, batch, input_size)\n",
    "    input_tensor, target_tensor = randomTrainingPair() # random embedded sentence\n",
    "    hidden = Variable(torch.zeros(num_layers, 1, hidden_size)) # for init only\n",
    "    loss = train(input_tensor, target_tensor, hidden)\n",
    "    \n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "    #if iter % plot_every == 0:\n",
    "     #   all_losses.append(total_loss / plot_every)\n",
    "     #   total_loss = 0\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 0s (1000 0%) ouranos\n",
      "0m 0s (2000 0%) minsky\n",
      "0m 0s (3000 0%) euro88\n",
      "0m 0s (4000 1%) giht\n",
      "0m 0s (5000 1%) tofte\n",
      "0m 0s (6000 1%) nyckelharpa\n",
      "0m 1s (7000 1%) angoras\n",
      "0m 1s (8000 2%) birlas\n",
      "0m 1s (9000 2%) haehnel\n",
      "0m 1s (10000 2%) algodones\n",
      "0m 1s (11000 2%) adr\n",
      "0m 1s (12000 3%) 1430gmt\n",
      "0m 2s (13000 3%) royal\n",
      "0m 2s (14000 3%) socio-historical\n",
      "0m 2s (15000 3%) ōsaka\n",
      "0m 2s (16000 4%) khandwa\n",
      "0m 2s (17000 4%) nakajima\n",
      "0m 2s (18000 4%) montanari\n",
      "0m 2s (19000 4%) geiseric\n",
      "0m 3s (20000 5%) kumārajīva\n",
      "0m 3s (21000 5%) chthonic\n",
      "0m 3s (22000 5%) sokol\n",
      "0m 3s (23000 5%) ruethling\n",
      "0m 3s (24000 6%) 1572\n",
      "0m 3s (25000 6%) overwinter\n",
      "0m 4s (26000 6%) who2004.svg\n",
      "0m 4s (27000 6%) lvovich\n",
      "0m 4s (28000 7%) liepājas\n",
      "0m 4s (29000 7%) towage\n",
      "0m 4s (30000 7%) skotnikov\n",
      "0m 4s (31000 7%) appleby-in-westmorland\n",
      "0m 4s (32000 8%) vadas\n",
      "0m 5s (33000 8%) tusker\n",
      "0m 5s (34000 8%) ond\n",
      "0m 5s (35000 8%) 25-50\n",
      "0m 5s (36000 9%) procyon\n",
      "0m 5s (37000 9%) trilateral\n",
      "0m 5s (38000 9%) corporately\n",
      "0m 6s (39000 9%) ridenhour\n",
      "0m 6s (40000 10%) rockettes\n",
      "0m 6s (41000 10%) naklo\n",
      "0m 6s (42000 10%) håvard\n",
      "0m 6s (43000 10%) dhenkanal\n",
      "0m 6s (44000 11%) gauges\n",
      "0m 6s (45000 11%) p3\n",
      "0m 7s (46000 11%) persuasive\n",
      "0m 7s (47000 11%) 777-200er\n",
      "0m 7s (48000 12%) paracels\n",
      "0m 7s (49000 12%) bharathan\n",
      "0m 7s (50000 12%) ewan\n",
      "0m 7s (51000 12%) papilio\n",
      "0m 8s (52000 13%) dutse\n",
      "0m 8s (53000 13%) milliliters\n",
      "0m 8s (54000 13%) 89.10\n",
      "0m 8s (55000 13%) pallida\n",
      "0m 8s (56000 14%) guyane\n",
      "0m 8s (57000 14%) mid-missouri\n",
      "0m 8s (58000 14%) flysch\n",
      "0m 9s (59000 14%) defencemen\n",
      "0m 9s (60000 15%) ḩoseynābād\n",
      "0m 9s (61000 15%) flicked\n",
      "0m 9s (62000 15%) tormented\n",
      "0m 9s (63000 15%) gulay\n",
      "0m 9s (64000 16%) poisonous\n",
      "0m 10s (65000 16%) potter\n",
      "0m 10s (66000 16%) 99.80\n",
      "0m 10s (67000 16%) blackmailing\n",
      "0m 10s (68000 17%) cahal\n",
      "0m 10s (69000 17%) artus\n",
      "0m 10s (70000 17%) cakobau\n",
      "0m 11s (71000 17%) 68.64\n",
      "0m 11s (72000 18%) rafaqat\n",
      "0m 11s (73000 18%) rundschau\n",
      "0m 11s (74000 18%) stampp\n",
      "0m 11s (75000 18%) cejudo\n",
      "0m 11s (76000 19%) eyeshield\n",
      "0m 11s (77000 19%) isobutane\n",
      "0m 12s (78000 19%) dodecahedral\n",
      "0m 12s (79000 19%) brushstroke\n",
      "0m 12s (80000 20%) c&nw\n",
      "0m 12s (81000 20%) maack\n",
      "0m 12s (82000 20%) 2,553\n",
      "0m 12s (83000 20%) plock\n",
      "0m 13s (84000 21%) mentoring\n",
      "0m 13s (85000 21%) doleac\n",
      "0m 13s (86000 21%) take-off\n",
      "0m 13s (87000 21%) regna\n",
      "0m 13s (88000 22%) willens\n",
      "0m 13s (89000 22%) 1,642\n",
      "0m 13s (90000 22%) oenothera\n",
      "0m 14s (91000 22%) vlb\n",
      "0m 14s (92000 23%) 107-89\n",
      "0m 14s (93000 23%) transmitter\n",
      "0m 14s (94000 23%) kasamatsu\n",
      "0m 14s (95000 23%) mjällby\n",
      "0m 14s (96000 24%) jansch\n",
      "0m 14s (97000 24%) reinject\n",
      "0m 15s (98000 24%) hydrolysate\n",
      "0m 15s (99000 24%) '80s\n",
      "0m 15s (100000 25%) south-east\n",
      "0m 15s (101000 25%) anthropology\n",
      "0m 15s (102000 25%) tuck\n",
      "0m 15s (103000 25%) teachers\n",
      "0m 15s (104000 26%) sajidul\n",
      "0m 16s (105000 26%) krstić\n",
      "0m 16s (106000 26%) westaway\n",
      "0m 16s (107000 26%) plumlee\n",
      "0m 16s (108000 27%) longarm\n",
      "0m 16s (109000 27%) auditorium\n",
      "0m 17s (110000 27%) buchholtz\n",
      "0m 17s (111000 27%) wellawatte\n",
      "0m 17s (112000 28%) ilf\n",
      "0m 17s (113000 28%) walta\n",
      "0m 17s (114000 28%) baudot\n",
      "0m 17s (115000 28%) feser\n",
      "0m 17s (116000 28%) u.s.-pakistani\n",
      "0m 18s (117000 29%) 108.84\n",
      "0m 18s (118000 29%) univalent\n",
      "0m 18s (119000 29%) tamina\n",
      "0m 18s (120000 30%) geomancy\n",
      "0m 18s (121000 30%) allegations\n",
      "0m 18s (122000 30%) sanofi-aventis\n",
      "0m 18s (123000 30%) ingrate\n",
      "0m 19s (124000 31%) méndez\n",
      "0m 19s (125000 31%) augmentative\n",
      "0m 19s (126000 31%) niazi\n",
      "0m 19s (127000 31%) 55819\n",
      "0m 19s (128000 32%) meatheads\n",
      "0m 19s (129000 32%) sefolosha\n",
      "0m 20s (130000 32%) 96-0\n",
      "0m 20s (131000 32%) wallstreet\n",
      "0m 20s (132000 33%) chakhoyan\n",
      "0m 20s (133000 33%) leaderless\n",
      "0m 20s (134000 33%) 24-team\n",
      "0m 20s (135000 33%) 11-track\n",
      "0m 21s (136000 34%) arden\n",
      "0m 21s (137000 34%) grödig\n",
      "0m 21s (138000 34%) kogo\n",
      "0m 21s (139000 34%) monzon\n",
      "0m 21s (140000 35%) d-nj\n",
      "0m 21s (141000 35%) chesed\n",
      "0m 21s (142000 35%) coelurosaur\n",
      "0m 22s (143000 35%) game1\n",
      "0m 22s (144000 36%) rakovsky\n",
      "0m 22s (145000 36%) pesäpallo\n",
      "0m 22s (146000 36%) maestrat\n",
      "0m 22s (147000 36%) sors\n",
      "0m 22s (148000 37%) bfit\n",
      "0m 23s (149000 37%) simón\n",
      "0m 23s (150000 37%) bhusan\n",
      "0m 23s (151000 37%) background_notes\n",
      "0m 23s (152000 38%) hongkongbank\n",
      "0m 23s (153000 38%) long-tailed\n",
      "0m 23s (154000 38%) khurshid\n",
      "0m 24s (155000 38%) abrasion\n",
      "0m 24s (156000 39%) detonation\n",
      "0m 24s (157000 39%) mccamant\n",
      "0m 24s (158000 39%) 3,517\n",
      "0m 24s (159000 39%) gramlich\n",
      "0m 24s (160000 40%) 6,370\n",
      "0m 24s (161000 40%) ligaments\n",
      "0m 25s (162000 40%) bamburi\n",
      "0m 25s (163000 40%) guilbeault\n",
      "0m 25s (164000 41%) sht\n",
      "0m 25s (165000 41%) 15:54\n",
      "0m 25s (166000 41%) alluringly\n",
      "0m 25s (167000 41%) harnwell\n",
      "0m 26s (168000 42%) mandalika\n",
      "0m 26s (169000 42%) sumatra\n",
      "0m 26s (170000 42%) 90.74\n",
      "0m 26s (171000 42%) في\n",
      "0m 26s (172000 43%) sanjabi\n",
      "0m 26s (173000 43%) birthdates\n",
      "0m 26s (174000 43%) ubol\n",
      "0m 27s (175000 43%) dingers\n",
      "0m 27s (176000 44%) boco\n",
      "0m 27s (177000 44%) samarra\n",
      "0m 27s (178000 44%) mv\n",
      "0m 27s (179000 44%) mordkin\n",
      "0m 27s (180000 45%) flom\n",
      "0m 27s (181000 45%) non-financial\n",
      "0m 28s (182000 45%) non-medal\n",
      "0m 28s (183000 45%) bollhuset\n",
      "0m 28s (184000 46%) sudikoff\n",
      "0m 28s (185000 46%) gariety\n",
      "0m 28s (186000 46%) foremost\n",
      "0m 29s (187000 46%) wayforward\n",
      "0m 29s (188000 47%) umana\n",
      "0m 29s (189000 47%) gahbauer\n",
      "0m 29s (190000 47%) mover\n",
      "0m 29s (191000 47%) brigte\n",
      "could not look up embedded word: float division by zero\n",
      "0m 29s (192000 48%) eight-season\n",
      "0m 29s (193000 48%) re-distribution\n",
      "0m 30s (194000 48%) sage-grouse\n",
      "0m 30s (195000 48%) hevia\n",
      "0m 30s (196000 49%) preindustrial\n",
      "0m 30s (197000 49%) proteinaceous\n",
      "0m 30s (198000 49%) rosner\n",
      "0m 30s (199000 49%) 74-76\n",
      "0m 31s (200000 50%) esfandiar\n",
      "0m 31s (201000 50%) scuzz\n",
      "0m 31s (202000 50%) compatibles\n",
      "0m 31s (203000 50%) lambrecht\n",
      "0m 31s (204000 51%) 2,240\n",
      "0m 31s (205000 51%) barreled\n",
      "0m 31s (206000 51%) jodhpurs\n",
      "0m 32s (207000 51%) appy\n",
      "0m 32s (208000 52%) neusiedler\n",
      "0m 32s (209000 52%) arp2/3\n",
      "0m 32s (210000 52%) noapara\n",
      "0m 32s (211000 52%) kreuz\n",
      "0m 32s (212000 53%) 130kg\n",
      "0m 32s (213000 53%) kalinago\n",
      "0m 33s (214000 53%) earrings\n",
      "0m 33s (215000 53%) 893-8751\n",
      "0m 33s (216000 54%) nestorian\n",
      "0m 33s (217000 54%) vidya\n",
      "0m 33s (218000 54%) ایران\n",
      "0m 33s (219000 54%) n'doye\n",
      "0m 33s (220000 55%) 1994-2004\n",
      "0m 34s (221000 55%) pandin\n",
      "0m 34s (222000 55%) baccalieri\n",
      "0m 34s (223000 55%) riverbed\n",
      "0m 34s (224000 56%) zhakiyanov\n",
      "0m 34s (225000 56%) 652\n",
      "0m 34s (226000 56%) ojos\n",
      "0m 35s (227000 56%) mutter\n",
      "0m 35s (228000 56%) 98-86\n",
      "0m 35s (229000 57%) mendel\n",
      "0m 35s (230000 57%) provides\n",
      "0m 35s (231000 57%) sarason\n",
      "0m 35s (232000 57%) detailers\n",
      "0m 35s (233000 58%) fhwa\n",
      "0m 36s (234000 58%) pageantry\n",
      "0m 36s (235000 58%) emts\n",
      "0m 36s (236000 59%) 1,360\n",
      "0m 36s (237000 59%) sangir\n",
      "0m 36s (238000 59%) odia\n",
      "0m 36s (239000 59%) forbade\n",
      "0m 37s (240000 60%) boto\n",
      "0m 37s (241000 60%) bowlers\n",
      "0m 37s (242000 60%) 9a-3p\n",
      "0m 37s (243000 60%) 13:00\n",
      "0m 37s (244000 61%) eight-man\n",
      "0m 37s (245000 61%) 5,248\n",
      "0m 38s (246000 61%) tchepalova\n",
      "0m 38s (247000 61%) wrongfully\n",
      "0m 38s (248000 62%) turntablists\n",
      "0m 38s (249000 62%) chaybasar-e\n",
      "0m 38s (250000 62%) topuzakov\n",
      "0m 38s (251000 62%) heusner\n",
      "0m 38s (252000 63%) tragically\n",
      "0m 39s (253000 63%) thomasschule\n",
      "0m 39s (254000 63%) veale\n",
      "0m 39s (255000 63%) apnewsnow\n",
      "0m 39s (256000 64%) maudsley\n",
      "0m 39s (257000 64%) larus\n",
      "0m 39s (258000 64%) rathcoole\n",
      "0m 40s (259000 64%) 6-footers\n",
      "0m 40s (260000 65%) battista\n",
      "0m 40s (261000 65%) 10.90\n",
      "0m 40s (262000 65%) under-used\n",
      "0m 40s (263000 65%) ingall\n",
      "0m 40s (264000 66%) 9.44\n",
      "0m 41s (265000 66%) 19.04\n",
      "0m 41s (266000 66%) repasky\n",
      "0m 41s (267000 66%) cognatic\n",
      "0m 41s (268000 67%) orma\n",
      "0m 41s (269000 67%) rothchild\n",
      "0m 41s (270000 67%) 8,800\n",
      "0m 42s (271000 67%) groomsman\n",
      "0m 42s (272000 68%) pipelaying\n",
      "0m 42s (273000 68%) 141-year\n",
      "0m 42s (274000 68%) mainers\n",
      "0m 42s (275000 68%) 47.01\n",
      "0m 42s (276000 69%) rocas\n",
      "0m 42s (277000 69%) 8,320\n",
      "0m 43s (278000 69%) berenger\n",
      "0m 43s (279000 69%) iwpr\n",
      "0m 43s (280000 70%) croup\n",
      "0m 43s (281000 70%) ricke\n",
      "0m 43s (282000 70%) assents\n",
      "0m 43s (283000 70%) truby\n",
      "0m 44s (284000 71%) 84.81\n",
      "0m 44s (285000 71%) co-designated\n",
      "0m 44s (286000 71%) onatopp\n",
      "0m 44s (287000 71%) resection\n",
      "0m 44s (288000 72%) finny\n",
      "0m 44s (289000 72%) lipman\n",
      "0m 45s (290000 72%) gaziosmanpasa\n",
      "0m 45s (291000 72%) 927,000\n",
      "0m 45s (292000 73%) atik\n",
      "0m 45s (293000 73%) hartanto\n",
      "0m 45s (294000 73%) c&t\n",
      "0m 45s (295000 73%) kōnan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 46s (296000 74%) slighter\n",
      "0m 46s (297000 74%) chronister\n",
      "0m 46s (298000 74%) 5:39\n",
      "0m 46s (299000 74%) ascochyta\n",
      "0m 46s (300000 75%) dvd-r\n",
      "0m 46s (301000 75%) funes\n",
      "0m 47s (302000 75%) chubbuck\n",
      "0m 47s (303000 75%) kluttz\n",
      "0m 47s (304000 76%) supermarkets\n",
      "0m 47s (305000 76%) rihl\n",
      "0m 47s (306000 76%) 20,444\n",
      "0m 47s (307000 76%) siddhis\n",
      "0m 48s (308000 77%) 700w\n",
      "0m 48s (309000 77%) anatoma\n",
      "0m 48s (310000 77%) pr-10\n",
      "0m 48s (311000 77%) khokhlova\n",
      "0m 48s (312000 78%) prizefighters\n",
      "0m 48s (313000 78%) 1.154\n",
      "0m 49s (314000 78%) magid\n",
      "0m 49s (315000 78%) soest\n",
      "0m 49s (316000 79%) ryoo\n",
      "0m 49s (317000 79%) 185.5\n",
      "0m 49s (318000 79%) fna\n",
      "0m 49s (319000 79%) gholston\n",
      "0m 50s (320000 80%) vorsah\n",
      "0m 50s (321000 80%) coriaceous\n",
      "0m 50s (322000 80%) govern\n",
      "0m 50s (323000 80%) bugey\n",
      "0m 50s (324000 81%) ramo\n",
      "0m 50s (325000 81%) 450-meter\n",
      "0m 50s (326000 81%) 21.83\n",
      "0m 51s (327000 81%) sariyev\n",
      "0m 51s (328000 82%) 048\n",
      "0m 51s (329000 82%) skouris\n",
      "0m 51s (330000 82%) amplifier\n",
      "0m 51s (331000 82%) instar\n",
      "0m 51s (332000 83%) chagas\n",
      "0m 51s (333000 83%) souliotes\n",
      "0m 52s (334000 83%) anti-evolution\n",
      "0m 52s (335000 83%) bickleigh\n",
      "0m 52s (336000 84%) afaf\n",
      "0m 52s (337000 84%) rysavy\n",
      "0m 52s (338000 84%) ælfwald\n",
      "0m 52s (339000 84%) tripp\n",
      "0m 52s (340000 85%) d'urfey\n",
      "0m 53s (341000 85%) fireproofed\n",
      "0m 53s (342000 85%) sylvatica\n",
      "0m 53s (343000 85%) rosell\n",
      "0m 53s (344000 86%) vinegars\n",
      "0m 53s (345000 86%) acetabulum\n",
      "0m 53s (346000 86%) keselowski\n",
      "0m 54s (347000 86%) christal\n",
      "0m 54s (348000 87%) fivefold\n",
      "0m 54s (349000 87%) honeychurch\n",
      "0m 54s (350000 87%) invesco\n",
      "0m 54s (351000 87%) cecin\n",
      "0m 54s (352000 88%) 183,500\n",
      "0m 54s (353000 88%) fleurette\n",
      "0m 55s (354000 88%) bhujbal\n",
      "0m 55s (355000 88%) darrington\n",
      "0m 55s (356000 89%) freinsheim\n",
      "0m 55s (357000 89%) inbal\n",
      "0m 55s (358000 89%) subtext\n",
      "0m 55s (359000 89%) zosteropidae\n",
      "0m 56s (360000 90%) mandrax\n",
      "0m 56s (361000 90%) 103-year\n",
      "0m 56s (362000 90%) 87,000\n",
      "0m 56s (363000 90%) phocaea\n",
      "0m 56s (364000 91%) eot\n",
      "0m 56s (365000 91%) eoi\n",
      "0m 57s (366000 91%) 9,800\n",
      "0m 57s (367000 91%) vertonghen\n",
      "0m 57s (368000 92%) officers\n",
      "0m 57s (369000 92%) rishiri\n",
      "0m 57s (370000 92%) hyeres\n",
      "0m 57s (371000 92%) severus\n",
      "0m 57s (372000 93%) naisbitt\n",
      "0m 58s (373000 93%) megalitres\n",
      "0m 58s (374000 93%) funt\n",
      "0m 58s (375000 93%) rhizome\n",
      "0m 58s (376000 94%) stagedoor\n",
      "0m 58s (377000 94%) 14-ranked\n",
      "0m 58s (378000 94%) hastinapur\n",
      "0m 59s (379000 94%) self-assertion\n",
      "0m 59s (380000 95%) promoter\n",
      "0m 59s (381000 95%) inbursa\n",
      "0m 59s (382000 95%) mlakar\n",
      "0m 59s (383000 95%) imogen\n",
      "0m 59s (384000 96%) wpta\n",
      "1m 0s (385000 96%) scholten\n",
      "1m 0s (386000 96%) makam\n",
      "1m 0s (387000 96%) bowlful\n",
      "1m 0s (388000 97%) germersheim\n",
      "1m 0s (389000 97%) lamno\n",
      "1m 0s (390000 97%) orăştie\n",
      "1m 0s (391000 97%) soininen\n",
      "1m 1s (392000 98%) huludao\n",
      "1m 1s (393000 98%) 72.12\n",
      "1m 1s (394000 98%) bakhyt\n",
      "1m 1s (395000 98%) pre-dreadnought\n",
      "1m 1s (396000 99%) crutchlow\n",
      "1m 1s (397000 99%) xms\n",
      "1m 2s (398000 99%) maculata\n",
      "1m 2s (399000 99%) center-west\n",
      "1m 2s (400000 100%) bahaji\n",
      "Output:  low mo95 bb96 k978-1 str95bb str95bb str95bb str95bb str95bb str95bb str95bb str95bb bb96 bb96 str95bb str95bb str95bb bb96 bb96 bb96 bb96 \n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "\n",
    "eos = torch.FloatTensor(1,300).zero_()\n",
    "\n",
    "def round_floats(float_num, dummy):\n",
    "    return float(\"{0:.2f}\".format(float_num))\n",
    "\n",
    "\n",
    "def generate():\n",
    "    # TODO pick random start word\n",
    "    input = Variable(torch.FloatTensor(embeddings_index[\"high\"]).view(1,1,300)) # start word \n",
    "    \n",
    "    generated_sentence = [input.data]\n",
    "    hidden = Variable(torch.zeros(num_layers, 1, hidden_size))\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        output, hidden = gru(input, hidden)\n",
    "        #print(type(output.data))\n",
    "        #topv, topi = output.data.topk(1)\n",
    "        #topi = topi[0][0]\n",
    "        #print(topi)\n",
    "        if torch.equal(output.data.view(1,300), eos):\n",
    "            print(\"found eos after {} words\".format(i))\n",
    "            break\n",
    "        else:\n",
    "            #embedded_word = all_letters[topi]\n",
    "            #next_input = Variable(torch.FloatTensor(output.data))\n",
    "            \n",
    "            generated_sentence.append(output.data)  # storing embedded words for now\n",
    "            \n",
    "            \n",
    "        input = output\n",
    "\n",
    "    return generated_sentence\n",
    "    \n",
    "\n",
    "print_every = 1000\n",
    "def translate(embedded_sentence):\n",
    "    iter = 0\n",
    "    start = time.time()\n",
    "    closest_matches = [[0.0,0,\"???\"]] * len(embedded_sentence)  # TODO \n",
    "        \n",
    "    # go through list of words in order of occurance probability\n",
    "    for vector, word in word_index.items():\n",
    "        iter += 1\n",
    "        if iter % print_every == 0:\n",
    "            print('%s (%d %d%%) %s' % (timeSince(start), iter, iter / 400000 * 100, word))\n",
    "            \n",
    "        #cursor_tensor_key = emb_word_tensor.map_(torch.FloatTensor([0]), round_floats)\n",
    "        \n",
    "        # check distance for each word in sentence\n",
    "        ind = 0\n",
    "        for emb_word_tensor in embedded_sentence:\n",
    "            try:\n",
    "                # calculate distance\n",
    "                \n",
    "                dist = 1.0 / torch.sum(torch.pow(vector.sub(emb_word_tensor), 2))\n",
    "                \n",
    "                if dist > closest_matches[ind][0]:\n",
    "                    \n",
    "                    closest_matches[ind] = [dist, vector, word]\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(\"could not look up embedded word: {}\".format(e))\n",
    "          \n",
    "            ind += 1\n",
    "    \n",
    "    sentence = \"\"\n",
    "    for [_, _, word] in closest_matches:\n",
    "        sentence = sentence + str(word) + \" \"\n",
    "                       \n",
    "    return sentence\n",
    "\n",
    "print(\"Output: \", translate(generate()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generated_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a21c85863d0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'generated_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "print(generated_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
